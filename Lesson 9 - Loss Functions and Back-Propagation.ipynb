{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show some common loss functions, then walk through through forward and backward passes in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions\n",
    "======\n",
    "\n",
    "$y_i =$ ith category\n",
    "\n",
    "$p_i =$ ith certainty that category i is found\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "L_1 = |\\vec{y} - \\vec{p}|\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "L_2 = (\\vec{y} - \\vec{p})^2\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{log} = - \\sum_{i=1}^N y_iLog(p_i)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, apply these to a multi-label classification problem.\n",
    "#Suppose we have a flower classifier with four categories: violet,\n",
    "#poppy, rose, empty.\n",
    "import torch\n",
    "import math\n",
    "torch.cuda.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yvec = torch.FloatTensor([0, 1 , 0, 0])\n",
    "Pvec = torch.FloatTensor([0.001, .5 , .1, .1]) \n",
    "PvecSmallError = torch.FloatTensor([0.001, .9 , .1, .1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First with a large error\n",
      "L1 Loss =  0.5196162050938572\n",
      "L2 Loss =  0.27000100059614146\n",
      "Log Loss =  0.6931471824645996\n",
      "Now with a small error\n",
      "L1 Loss =  0.17320798296993586\n",
      "L2 Loss =  0.030001005364513594\n",
      "Log Loss =  0.10536054521799088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "print('First with a large error')\n",
    "#\n",
    "L1Loss=(Yvec-Pvec).norm()\n",
    "print('L1 Loss = ',L1Loss)\n",
    "L2Loss=math.pow((Yvec-Pvec).norm(),2)\n",
    "print('L2 Loss = ',L2Loss)\n",
    "LogLoss = (-Yvec*Pvec.log()).sum()\n",
    "print('Log Loss = ',LogLoss)\n",
    "\n",
    "#\n",
    "print('Now with a small error')\n",
    "#\n",
    "L1Loss=(Yvec-PvecSmallError).norm()\n",
    "print('L1 Loss = ',L1Loss)\n",
    "L2Loss=math.pow((Yvec-PvecSmallError).norm(),2)\n",
    "print('L2 Loss = ',L2Loss)\n",
    "LogLoss = (-Yvec*PvecSmallError.log()).sum()\n",
    "print('Log Loss = ',LogLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates a couple of important behaviors\n",
    "L2 loss places a very heavy weight on outliers as compared to L1 loss\n",
    "Log loss penalizes highly incorrect values even more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Gradient Calculation - How Does It Work?\n",
    "======\n",
    "Pytorch uses parameters which retain their last value\n",
    "\n",
    "Lets follow a calculation forward and backward to see how this works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb\n",
    "\n",
    "#\n",
    "# During learning, the input will be changed resulting in some change of the loss\n",
    "# That change is simulated with two predictions Pvec and PvecLowerLoss.  \n",
    "# The delta will be Loss - LowerLoss\n",
    "#\n",
    "Yvec = torch.FloatTensor([0, 1 , 0, 0])\n",
    "Pvec = torch.FloatTensor([0.001, .5 , .1, .1]) \n",
    "PvecLowerLoss = torch.FloatTensor([0.001, .55 , .1, .1]) \n",
    "\n",
    "Yvar = Variable(Yvec,requires_grad=True)\n",
    "Pvar = Variable(Pvec,requires_grad=True)\n",
    "PvarLowerLoss = Variable(PvecLowerLoss,requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate a training step\n",
    "L2LossVar = Variable.pow((Yvar-Pvar).norm(),2)\n",
    "L2LossVarLowerLoss = Variable.pow((Yvar-PvarLowerLoss).norm(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-02 *\n",
      "  4.7500\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The is the change in loss from the simulated step\n",
    "LossDelta=(L2LossVar.data - L2LossVarLowerLoss.data)\n",
    "print(LossDelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation\n",
    "======\n",
    "Passing the loss change to the output of the calculation will cause all derivatives to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2LossVarLowerLoss.backward(gradient=LossDelta, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  0.0095\n",
      " -4.2750\n",
      "  0.9500\n",
      "  0.9500\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstGradientResult = PvarLowerLoss.grad.clone()\n",
    "print(firstGradientResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is the intermediate data stored, and how is it used?\n",
    "======\n",
    "In the case of built-in functions like subtract and pow, it looks\n",
    "like the backward pass is in C code and inaccessible to Python\n",
    "\n",
    "To get a better feel for this, define a PyTorch method to square something, \n",
    "and use that in place of Variable.pow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "import pdb\n",
    "class square(Function):\n",
    "        \n",
    "             @staticmethod\n",
    "             def forward(ctx, i):\n",
    "                result = i.pow(2)\n",
    "                ctx.save_for_backward(result)\n",
    "                return result\n",
    "        \n",
    "             @staticmethod\n",
    "             def backward(ctx, grad_output):\n",
    "                 result, = ctx.saved_variables\n",
    "                 return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L2LossCustom = square.apply((Yvar-PvarLowerLoss).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.2225\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.2225\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check that the forward pass has created the same result\n",
    "print(L2LossVarLowerLoss)\n",
    "print(L2LossCustom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-02 *\n",
      "  4.7500\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(LossDelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PvarLowerLoss.grad.data.zero_()\n",
    "L2LossCustom.backward(gradient=LossDelta, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  0.0101\n",
       " -4.5315\n",
       "  1.0070\n",
       "  1.0070\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PvarLowerLoss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  0.0095\n",
       " -4.2750\n",
       "  0.9500\n",
       "  0.9500\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# The above result should match the first gradient calculation since all that has happened\n",
    "# is replacing a PyTorch internal function with one defined in Python\n",
    "#\n",
    "\n",
    "firstGradientResult\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is nearly (but not quite) identical.  Still need to figure out why.\n",
    "What happens if the gradients are not zeroed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  0.0201\n",
       " -9.0630\n",
       "  2.0140\n",
       "  2.0140\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculation with the gradient not zeroed\n",
    "L2LossCustom.backward(gradient=LossDelta, retain_graph=True)\n",
    "PvarLowerLoss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0003\n",
       "-0.1359\n",
       " 0.0302\n",
       " 0.0302\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2LossCustom.backward(gradient=LossDelta, retain_graph=True)\n",
    "PvarLowerLoss.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration simply add the gradient into the .grad field.  The gradient is accumulated.\n",
    "The purpose is to facilitate averaging the gradient over an entire minibatch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
